{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d0bd39f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "26b183d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!curl -O http://files.grouplens.org/datasets/movielens/ml-100k.zip\n",
    "#!unzip ml-100k.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "21f4f129",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xoejang\\Documents\\tek2\\ml-100k\n"
     ]
    }
   ],
   "source": [
    "cd ml-100k/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "653c68b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Volume in drive C has no label.\n",
      " Volume Serial Number is 0A1D-E648\n",
      "\n",
      " Directory of C:\\Users\\Xoejang\\Documents\\tek2\\ml-100k\n",
      "\n",
      "01/30/2016  03:26 AM    <DIR>          .\n",
      "07/21/2022  10:35 PM    <DIR>          ..\n",
      "07/20/2000  04:09 AM               716 allbut.pl\n",
      "07/20/2000  04:09 AM               643 mku.sh\n",
      "01/30/2016  03:26 AM             6,750 README\n",
      "07/20/2000  04:09 AM         1,979,173 u.data\n",
      "07/20/2000  04:09 AM               202 u.genre\n",
      "07/20/2000  04:09 AM                36 u.info\n",
      "07/20/2000  04:09 AM           236,344 u.item\n",
      "07/20/2000  04:09 AM               193 u.occupation\n",
      "07/20/2000  04:09 AM            22,628 u.user\n",
      "03/09/2001  01:33 AM         1,586,544 u1.base\n",
      "03/09/2001  01:32 AM           392,629 u1.test\n",
      "03/09/2001  01:33 AM         1,583,948 u2.base\n",
      "03/09/2001  01:33 AM           395,225 u2.test\n",
      "03/09/2001  01:33 AM         1,582,546 u3.base\n",
      "03/09/2001  01:33 AM           396,627 u3.test\n",
      "03/09/2001  01:33 AM         1,581,878 u4.base\n",
      "03/09/2001  01:33 AM           397,295 u4.test\n",
      "03/09/2001  01:34 AM         1,581,776 u5.base\n",
      "03/09/2001  01:33 AM           397,397 u5.test\n",
      "03/09/2001  01:34 AM         1,792,501 ua.base\n",
      "03/09/2001  01:34 AM           186,672 ua.test\n",
      "03/09/2001  01:34 AM         1,792,476 ub.base\n",
      "03/09/2001  01:34 AM           186,697 ub.test\n",
      "              23 File(s)     16,100,896 bytes\n",
      "               2 Dir(s)  689,853,284,352 bytes free\n"
     ]
    }
   ],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f3d6e227",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "196\t242\t3\t881250949\n",
      "186\t302\t3\t891717742\n",
      "22\t377\t1\t878887116\n",
      "244\t51\t2\t880606923\n",
      "166\t346\t1\t886397596\n",
      "298\t474\t4\t884182806\n",
      "115\t265\t2\t881171488\n",
      "253\t465\t5\t891628467\n",
      "305\t451\t3\t886324817\n",
      "6\t86\t3\t883603013\n",
      "# line break\n",
      "100000 u.data\n"
     ]
    }
   ],
   "source": [
    "!head u.data\n",
    "!echo # line break\n",
    "!wc -l u.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3cd286b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>item_id</th>\n",
       "      <th>rating</th>\n",
       "      <th>timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>196</td>\n",
       "      <td>242</td>\n",
       "      <td>3</td>\n",
       "      <td>881250949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>186</td>\n",
       "      <td>302</td>\n",
       "      <td>3</td>\n",
       "      <td>891717742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>22</td>\n",
       "      <td>377</td>\n",
       "      <td>1</td>\n",
       "      <td>878887116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>244</td>\n",
       "      <td>51</td>\n",
       "      <td>2</td>\n",
       "      <td>880606923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>166</td>\n",
       "      <td>346</td>\n",
       "      <td>1</td>\n",
       "      <td>886397596</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user_id  item_id  rating  timestamp\n",
       "0      196      242       3  881250949\n",
       "1      186      302       3  891717742\n",
       "2       22      377       1  878887116\n",
       "3      244       51       2  880606923\n",
       "4      166      346       1  886397596"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "names = ['user_id', 'item_id', 'rating', 'timestamp']\n",
    "df = pd.read_csv('u.data', sep='\\t', names=names)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dd506197",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "943users\n",
      "1682items\n"
     ]
    }
   ],
   "source": [
    "n_users = df.user_id.unique().shape[0]\n",
    "n_items = df.item_id.unique().shape[0]\n",
    "print (str(n_users) + 'users')\n",
    "print (str(n_items) + 'items')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5ebc11f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[5., 3., 4., ..., 0., 0., 0.],\n",
       "       [4., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [5., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 5., 0., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ratings = np.zeros((n_users, n_items))\n",
    "for row in df.itertuples():\n",
    "    ratings[row[1]-1, row[2]-1] = row[3]\n",
    "ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "36fce554",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[5., 3., 4., ..., 0., 0., 0.],\n",
       "       [4., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [5., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 5., 0., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ratings = np.zeros((n_users, n_items))\n",
    "for row in df.itertuples():\n",
    "    ratings[row[1]-1, row[2]-1] = row[3]\n",
    "ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9313b4b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sparsity: 6.30%\n"
     ]
    }
   ],
   "source": [
    "sparsity = float(len(ratings.nonzero()[0]))\n",
    "sparsity /= (ratings.shape[0] * ratings.shape[1])\n",
    "sparsity *= 100\n",
    "print('sparsity: {:4.2f}%'.format(sparsity))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "26cf4ef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_split(ratings):\n",
    "    test = np.zeros(ratings.shape)\n",
    "    train = ratings.copy()\n",
    "    for user in range(ratings.shape[0]):\n",
    "        test_ratings = np.random.choice(ratings[user, :].nonzero()[0], \n",
    "                                        size=10, \n",
    "                                        replace=False)\n",
    "        train[user, test_ratings] = 0.\n",
    "        test[user, test_ratings] = ratings[user, test_ratings]\n",
    "        \n",
    "    # Test and training are truly disjoint\n",
    "    assert(np.all((train * test) == 0)) \n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "930d5ab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = train_test_split(ratings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0d325178",
   "metadata": {},
   "outputs": [],
   "source": [
    "def slow_similarity(ratings, kind='user'):\n",
    "    if kind == 'user':\n",
    "        axmax = 0\n",
    "        axmin = 1\n",
    "    elif kind == 'item':\n",
    "        axmax = 1\n",
    "        axmin = 0\n",
    "    sim = np.zeros((ratings.shape[axmax], ratings.shape[axmax]))\n",
    "    for u in xrange(ratings.shape[axmax]):\n",
    "        for uprime in xrange(ratings.shape[axmax]):\n",
    "            rui_sqrd = 0.\n",
    "            ruprimei_sqrd = 0.\n",
    "            for i in xrange(ratings.shape[axmin]):\n",
    "                sim[u, uprime] = ratings[u, i] * ratings[uprime, i]\n",
    "                rui_sqrd += ratings[u, i] ** 2\n",
    "                ruprimei_sqrd += ratings[uprime, i] ** 2\n",
    "            sim[u, uprime] /= rui_sqrd * ruprimei_sqrd\n",
    "    return sim\n",
    "\n",
    "def fast_similarity(ratings, kind='user', epsilon=1e-9):\n",
    "    # epsilon -> small number for handling dived-by-zero errors\n",
    "    if kind == 'user':\n",
    "        sim = ratings.dot(ratings.T) + epsilon\n",
    "    elif kind == 'item':\n",
    "        sim = ratings.T.dot(ratings) + epsilon\n",
    "    norms = np.array([np.sqrt(np.diagonal(sim))])\n",
    "    return (sim / norms / norms.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e9cb943c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%timeit slow_user_similarity(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2d243c6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "74.4 ms ± 5 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit fast_similarity(train, kind='user')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c8a2dc84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.         0.41164706 0.31333912 0.46684839]\n",
      " [0.41164706 1.         0.26971083 0.49139605]\n",
      " [0.31333912 0.26971083 1.         0.33716712]\n",
      " [0.46684839 0.49139605 0.33716712 1.        ]]\n"
     ]
    }
   ],
   "source": [
    "user_similarity = fast_similarity(train, kind='user')\n",
    "item_similarity = fast_similarity(train, kind='item')\n",
    "print(item_similarity[:4, :4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "443e07cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_slow_simple(ratings, similarity, kind='user'):\n",
    "    pred = np.zeros(ratings.shape)\n",
    "    if kind == 'user':\n",
    "        for i in range(ratings.shape[0]):\n",
    "            for j in range(ratings.shape[1]):\n",
    "                pred[i, j] = similarity[i, :].dot(ratings[:, j])\\\n",
    "                             /np.sum(np.abs(similarity[i, :]))\n",
    "        return pred\n",
    "    elif kind == 'item':\n",
    "        for i in range(ratings.shape[0]):\n",
    "            for j in range(ratings.shape[1]):\n",
    "                pred[i, j] = similarity[j, :].dot(ratings[i, :].T)\\\n",
    "                             /np.sum(np.abs(similarity[j, :]))\n",
    "\n",
    "        return pred\n",
    "\n",
    "def predict_fast_simple(ratings, similarity, kind='user'):\n",
    "    if kind == 'user':\n",
    "        return similarity.dot(ratings) / np.array([np.abs(similarity).sum(axis=1)]).T\n",
    "    elif kind == 'item':\n",
    "        return ratings.dot(similarity) / np.array([np.abs(similarity).sum(axis=1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c0431f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit predict_slow_simple(train, user_similarity, kind='user')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7ee7218",
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit predict_fast_simple(train, user_similarity, kind='user')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5edef2a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "def get_mse(pred, actual):\n",
    "    # Ignore nonzero terms.\n",
    "    pred = pred[actual.nonzero()].flatten()\n",
    "    actual = actual[actual.nonzero()].flatten()\n",
    "    return mean_squared_error(pred, actual)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f8b28f0",
   "metadata": {},
   "source": [
    "item_prediction = predict_fast_simple(train, item_similarity, kind='item')\n",
    "user_prediction = predict_fast_simple(train, user_similarity, kind='user')\n",
    "\n",
    "print 'User-based CF MSE: ' + str(get_mse(user_prediction, test))\n",
    "print 'Item-based CF MSE: ' + str(get_mse(item_prediction, test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3eef8ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "item_prediction = predict_fast_simple(train, item_similarity, kind='item')\n",
    "user_prediction = predict_fast_simple(train, user_similarity, kind='user')\n",
    "\n",
    "print('User-based CF MSE: ' + str(get_mse(user_prediction, test)))\n",
    "print ('Item-based CF MSE: ' + str(get_mse(item_prediction, test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb4d32fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_topk(ratings, similarity, kind='user', k=40):\n",
    "    pred = np.zeros(ratings.shape)\n",
    "    if kind == 'user':\n",
    "        for i in range(ratings.shape[0]):\n",
    "            top_k_users = [np.argsort(similarity[:,i])[:-k-1:-1]]\n",
    "            for j in range(ratings.shape[1]):\n",
    "                pred[i, j] = similarity[i, :][top_k_users].dot(ratings[:, j][top_k_users]) \n",
    "                pred[i, j] /= np.sum(np.abs(similarity[i, :][top_k_users]))\n",
    "    if kind == 'item':\n",
    "        for j in range(ratings.shape[1]):\n",
    "            top_k_items = [np.argsort(similarity[:,j])[:-k-1:-1]]\n",
    "            for i in range(ratings.shape[0]):\n",
    "                pred[i, j] = similarity[j, :][top_k_items].dot(ratings[i, :][top_k_items].T) \n",
    "                pred[i, j] /= np.sum(np.abs(similarity[j, :][top_k_items]))        \n",
    "    \n",
    "    return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6843fc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = predict_topk(train, user_similarity, kind='user', k=40)\n",
    "print (f'Top-k User-based CF MSE: {get_mse(pred, test)}')\n",
    "\n",
    "pred = predict_topk(train, item_similarity, kind='item', k=40)\n",
    "print (f'Top-k Item-based CF MSE: {get_mse(pred, test)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27ab35bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "k_array = [5, 15, 30, 50, 100, 200]\n",
    "user_train_mse = []\n",
    "user_test_mse = []\n",
    "item_test_mse = []\n",
    "item_train_mse = []\n",
    "\n",
    "def get_mse(pred, actual):\n",
    "    pred = pred[actual.nonzero()].flatten()\n",
    "    actual = actual[actual.nonzero()].flatten()\n",
    "    return mean_squared_error(pred, actual)\n",
    "\n",
    "for k in k_array:\n",
    "    user_pred = predict_topk(train, user_similarity, kind='user', k=k)\n",
    "    item_pred = predict_topk(train, item_similarity, kind='item', k=k)\n",
    "    \n",
    "    user_train_mse += [get_mse(user_pred, train)]\n",
    "    user_test_mse += [get_mse(user_pred, test)]\n",
    "    \n",
    "    item_train_mse += [get_mse(item_pred, train)]\n",
    "    item_test_mse += [get_mse(item_pred, test)]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6245368f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "\n",
    "pal = sns.color_palette(\"Set2\", 2)\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.plot(k_array, user_train_mse, c=pal[0], label='User-based train', alpha=0.5, linewidth=5)\n",
    "plt.plot(k_array, user_test_mse, c=pal[0], label='User-based test', linewidth=5)\n",
    "plt.plot(k_array, item_train_mse, c=pal[1], label='Item-based train', alpha=0.5, linewidth=5)\n",
    "plt.plot(k_array, item_test_mse, c=pal[1], label='Item-based test', linewidth=5)\n",
    "plt.legend(loc='best', fontsize=20)\n",
    "plt.xticks(fontsize=16);\n",
    "plt.yticks(fontsize=16);\n",
    "plt.xlabel('k', fontsize=30);\n",
    "plt.ylabel('MSE', fontsize=30);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f09c7ffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_nobias(ratings, similarity, kind='user'):\n",
    "    if kind == 'user':\n",
    "        user_bias = ratings.mean(axis=1)\n",
    "        ratings = (ratings - user_bias[:, np.newaxis]).copy()\n",
    "        pred = similarity.dot(ratings) / np.array([np.abs(similarity).sum(axis=1)]).T\n",
    "        pred += user_bias[:, np.newaxis]\n",
    "    elif kind == 'item':\n",
    "        item_bias = ratings.mean(axis=0)\n",
    "        ratings = (ratings - item_bias[np.newaxis, :]).copy()\n",
    "        pred = ratings.dot(similarity) / np.array([np.abs(similarity).sum(axis=1)])\n",
    "        pred += item_bias[np.newaxis, :]\n",
    "        \n",
    "    return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69273f9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_pred = predict_nobias(train, user_similarity, kind='user')\n",
    "print (f'Bias-subtracted User-based CF MSE: {get_mse(user_pred, test)}')\n",
    "\n",
    "item_pred = predict_nobias(train, item_similarity, kind='item')\n",
    "print (f'Bias-subtracted Item-based CF MSE: {get_mse(item_pred, test)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c91b57c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_topk_nobias(ratings, similarity, kind='user', k=40):\n",
    "    pred = np.zeros(ratings.shape)\n",
    "    if kind == 'user':\n",
    "        user_bias = ratings.mean(axis=1)\n",
    "        ratings = (ratings - user_bias[:, np.newaxis]).copy()\n",
    "        for i in range(ratings.shape[0]):\n",
    "            top_k_users = [np.argsort(similarity[:,i])[:-k-1:-1]]\n",
    "            for j in range(ratings.shape[1]):\n",
    "                pred[i, j] = similarity[i, :][top_k_users].dot(ratings[:, j][top_k_users]) \n",
    "                pred[i, j] /= np.sum(np.abs(similarity[i, :][top_k_users]))\n",
    "        pred += user_bias[:, np.newaxis]\n",
    "    if kind == 'item':\n",
    "        item_bias = ratings.mean(axis=0)\n",
    "        ratings = (ratings - item_bias[np.newaxis, :]).copy()\n",
    "        for j in range(ratings.shape[1]):\n",
    "            top_k_items = [np.argsort(similarity[:,j])[:-k-1:-1]]\n",
    "            for i in range(ratings.shape[0]):\n",
    "                pred[i, j] = similarity[j, :][top_k_items].dot(ratings[i, :][top_k_items].T) \n",
    "                pred[i, j] /= np.sum(np.abs(similarity[j, :][top_k_items])) \n",
    "        pred += item_bias[np.newaxis, :]\n",
    "        \n",
    "    return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fe62597",
   "metadata": {},
   "outputs": [],
   "source": [
    "k_array = [5, 15, 30, 50, 100, 200]\n",
    "user_train_mse = []\n",
    "user_test_mse = []\n",
    "item_test_mse = []\n",
    "item_train_mse = []\n",
    "\n",
    "for k in k_array:\n",
    "    user_pred = predict_topk_nobias(train, user_similarity, kind='user', k=k)\n",
    "    item_pred = predict_topk_nobias(train, item_similarity, kind='item', k=k)\n",
    "    \n",
    "    user_train_mse += [get_mse(user_pred, train)]\n",
    "    user_test_mse += [get_mse(user_pred, test)]\n",
    "    \n",
    "    item_train_mse += [get_mse(item_pred, train)]\n",
    "    item_test_mse += [get_mse(item_pred, test)]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1994a727",
   "metadata": {},
   "outputs": [],
   "source": [
    "pal = sns.color_palette(\"Set2\", 2)\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.plot(k_array, user_train_mse, c=pal[0], label='User-based train', alpha=0.5, linewidth=5)\n",
    "plt.plot(k_array, user_test_mse, c=pal[0], label='User-based test', linewidth=5)\n",
    "plt.plot(k_array, item_train_mse, c=pal[1], label='Item-based train', alpha=0.5, linewidth=5)\n",
    "plt.plot(k_array, item_test_mse, c=pal[1], label='Item-based test', linewidth=5)\n",
    "plt.legend(loc='best', fontsize=20)\n",
    "plt.xticks(fontsize=16);\n",
    "plt.yticks(fontsize=16);\n",
    "plt.xlabel('k', fontsize=30);\n",
    "plt.ylabel('MSE', fontsize=30);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc602e38",
   "metadata": {},
   "outputs": [],
   "source": [
    "!head -5 u.item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7aac316",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "response = requests.get('https://www.imdb.com/title/tt0114709/')\n",
    "print (response.url.split('/')[-2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc99b2c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get base url filepath structure. w185 corresponds to size of movie poster.\n",
    "headers = {'Accept': 'application/json'}\n",
    "payload = {'api_key': '5379c85bb32618a1d97ef0eaefe7aeed'} \n",
    "response = requests.get(\"http://api.themoviedb.org/3/configuration\", params=payload, headers=headers)\n",
    "response = json.loads(response.text)\n",
    "base_url = response['images']['base_url'] + 'w185'\n",
    "\n",
    "def get_poster(imdb_url, base_url):\n",
    "    # Get IMDB movie ID\n",
    "    response = requests.get('https://www.imdb.com/title/tt0114709/')\n",
    "    movie_id = response.url.split('/')[-2]\n",
    "    \n",
    "    # Query themoviedb.org API for movie poster path.\n",
    "    movie_url = 'http://api.themoviedb.org/3/movie/{:}/images'.format(movie_id)\n",
    "    headers = {'Accept': 'application/json'}\n",
    "    payload = {'api_key': '5379c85bb32618a1d97ef0eaefe7aeed'} \n",
    "    response = requests.get(movie_url, params=payload, headers=headers)\n",
    "    try:\n",
    "        file_path = json.loads(response.text)['posters'][0]['file_path']\n",
    "    except:\n",
    "        # IMDB movie ID is sometimes no good. Need to get correct one.\n",
    "        movie_title = imdb_url.split('?')[-1].split('(')[0]\n",
    "        payload['query'] = movie_title\n",
    "        response = requests.get('http://api.themoviedb.org/3/search/movie', params=payload, headers=headers)\n",
    "        movie_id = json.loads(response.text)['results'][0]['id']\n",
    "        payload.pop('query', None)\n",
    "        movie_url = 'http://api.themoviedb.org/3/movie/{:}/images'.format(movie_id)\n",
    "        response = requests.get(movie_url, params=payload, headers=headers)\n",
    "        file_path = json.loads(response.text)['posters'][0]['file_path']\n",
    "        \n",
    "    return base_url + file_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "877a3223",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "from IPython.display import display\n",
    "\n",
    "toy_story = 'https://www.imdb.com/title/tt0114709/'\n",
    "Image(url=get_poster(toy_story, base_url))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba1ed348",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in movie data\n",
    "idx_to_movie = {}\n",
    "with open('u.item', 'r') as f:\n",
    "    for line in f.readlines():\n",
    "        info = line.split('|')\n",
    "        idx_to_movie[int(info[0])-1] = info[4]\n",
    "        \n",
    "def top_k_movies(similarity, mapper, movie_idx, k=6):\n",
    "    return [mapper[x] for x in np.argsort(similarity[movie_idx,:])[:-k-1:-1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3f7ff32",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 0 # Toy Story\n",
    "movies = top_k_movies(item_similarity, idx_to_movie, idx)\n",
    "posters = tuple(Image(url=get_poster(movie, base_url)) for movie in movies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "107963fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(*posters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e273c0c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 1 # GoldenEye\n",
    "movies = top_k_movies(item_similarity, idx_to_movie, idx)\n",
    "posters = tuple(Image(url=get_poster(movie, base_url)) for movie in movies)\n",
    "display(*posters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c798e8ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get base url filepath structure. w185 corresponds to size of movie poster.\n",
    "headers = {'Accept': 'application/json'}\n",
    "payload = {'api_key': '5379c85bb32618a1d97ef0eaefe7aeed'} \n",
    "response = requests.get(\"http://api.themoviedb.org/3/configuration\", params=payload, headers=headers)\n",
    "response = json.loads(response.text)\n",
    "base_url = response['images']['base_url'] + 'w185'\n",
    "\n",
    "def get_poster(imdb_url, base_url):\n",
    "    # Get IMDB movie ID\n",
    "    response = requests.get('https://www.imdb.com/title/tt0113189/')\n",
    "    movie_id = response.url.split('/')[-2]\n",
    "    \n",
    "    # Query themoviedb.org API for movie poster path.\n",
    "    movie_url = 'http://api.themoviedb.org/3/movie/{:}/images'.format(movie_id)\n",
    "    headers = {'Accept': 'application/json'}\n",
    "    payload = {'api_key': '5379c85bb32618a1d97ef0eaefe7aeed'} \n",
    "    response = requests.get(movie_url, params=payload, headers=headers)\n",
    "    try:\n",
    "        file_path = json.loads(response.text)['posters'][0]['file_path']\n",
    "    except:\n",
    "        # IMDB movie ID is sometimes no good. Need to get correct one.\n",
    "        movie_title = imdb_url.split('?')[-1].split('(')[0]\n",
    "        payload['query'] = movie_title\n",
    "        response = requests.get('http://api.themoviedb.org/3/search/movie', params=payload, headers=headers)\n",
    "        movie_id = json.loads(response.text)['results'][0]['id']\n",
    "        payload.pop('query', None)\n",
    "        movie_url = 'http://api.themoviedb.org/3/movie/{:}/images'.format(movie_id)\n",
    "        response = requests.get(movie_url, params=payload, headers=headers)\n",
    "        file_path = json.loads(response.text)['posters'][0]['file_path']\n",
    "        \n",
    "    return base_url + file_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e588c6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in movie data\n",
    "idx_to_movie = {}\n",
    "with open('u.item', 'r') as f:\n",
    "    for line in f.readlines():\n",
    "        info = line.split('|')\n",
    "        idx_to_movie[int(info[0])-1] = info[4]\n",
    "        \n",
    "def top_k_movies(similarity, mapper, movie_idx, k=6):\n",
    "    return [mapper[x] for x in np.argsort(similarity[movie_idx,:])[:-k-1:-1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6969123c",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 1 # GoldenEye\n",
    "movies = top_k_movies(item_similarity, idx_to_movie, idx)\n",
    "posters = tuple(Image(url=get_poster(movie, base_url)) for movie in movies)\n",
    "display(*posters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fc3d6ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get base url filepath structure. w185 corresponds to size of movie poster.\n",
    "headers = {'Accept': 'application/json'}\n",
    "payload = {'api_key': '5379c85bb32618a1d97ef0eaefe7aeed'} \n",
    "response = requests.get(\"http://api.themoviedb.org/3/configuration\", params=payload, headers=headers)\n",
    "response = json.loads(response.text)\n",
    "base_url = response['images']['base_url'] + 'w185'\n",
    "\n",
    "def get_poster(imdb_url, base_url):\n",
    "    # Get IMDB movie ID\n",
    "    response = requests.get('https://www.imdb.com/title/tt0117110/')\n",
    "    movie_id = response.url.split('/')[-2]\n",
    "    \n",
    "    # Query themoviedb.org API for movie poster path.\n",
    "    movie_url = 'http://api.themoviedb.org/3/movie/{:}/images'.format(movie_id)\n",
    "    headers = {'Accept': 'application/json'}\n",
    "    payload = {'api_key': '5379c85bb32618a1d97ef0eaefe7aeed'} \n",
    "    response = requests.get(movie_url, params=payload, headers=headers)\n",
    "    try:\n",
    "        file_path = json.loads(response.text)['posters'][0]['file_path']\n",
    "    except:\n",
    "        # IMDB movie ID is sometimes no good. Need to get correct one.\n",
    "        movie_title = imdb_url.split('?')[-1].split('(')[0]\n",
    "        payload['query'] = movie_title\n",
    "        response = requests.get('http://api.themoviedb.org/3/search/movie', params=payload, headers=headers)\n",
    "        movie_id = json.loads(response.text)['results'][0]['id']\n",
    "        payload.pop('query', None)\n",
    "        movie_url = 'http://api.themoviedb.org/3/movie/{:}/images'.format(movie_id)\n",
    "        response = requests.get(movie_url, params=payload, headers=headers)\n",
    "        file_path = json.loads(response.text)['posters'][0]['file_path']\n",
    "        \n",
    "    return base_url + file_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e057443e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in movie data\n",
    "idx_to_movie = {}\n",
    "with open('u.item', 'r') as f:\n",
    "    for line in f.readlines():\n",
    "        info = line.split('|')\n",
    "        idx_to_movie[int(info[0])-1] = info[4]\n",
    "        \n",
    "def top_k_movies(similarity, mapper, movie_idx, k=6):\n",
    "    return [mapper[x] for x in np.argsort(similarity[movie_idx,:])[:-k-1:-1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "436718ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 20 # Muppet Treasure Island\n",
    "movies = top_k_movies(item_similarity, idx_to_movie, idx)\n",
    "posters = tuple(Image(url=get_poster(movie, base_url)) for movie in movies)\n",
    "display(*posters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01f8c70b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get base url filepath structure. w185 corresponds to size of movie poster.\n",
    "headers = {'Accept': 'application/json'}\n",
    "payload = {'api_key': '5379c85bb32618a1d97ef0eaefe7aeed'} \n",
    "response = requests.get(\"http://api.themoviedb.org/3/configuration\", params=payload, headers=headers)\n",
    "response = json.loads(response.text)\n",
    "base_url = response['images']['base_url'] + 'w185'\n",
    "\n",
    "def get_poster(imdb_url, base_url):\n",
    "    # Get IMDB movie ID\n",
    "    response = requests.get('https://www.imdb.com/title/tt0112508/')\n",
    "    movie_id = response.url.split('/')[-2]\n",
    "    \n",
    "    # Query themoviedb.org API for movie poster path.\n",
    "    movie_url = 'http://api.themoviedb.org/3/movie/{:}/images'.format(movie_id)\n",
    "    headers = {'Accept': 'application/json'}\n",
    "    payload = {'api_key': '5379c85bb32618a1d97ef0eaefe7aeed'} \n",
    "    response = requests.get(movie_url, params=payload, headers=headers)\n",
    "    try:\n",
    "        file_path = json.loads(response.text)['posters'][0]['file_path']\n",
    "    except:\n",
    "        # IMDB movie ID is sometimes no good. Need to get correct one.\n",
    "        movie_title = imdb_url.split('?')[-1].split('(')[0]\n",
    "        payload['query'] = movie_title\n",
    "        response = requests.get('http://api.themoviedb.org/3/search/movie', params=payload, headers=headers)\n",
    "        movie_id = json.loads(response.text)['results'][0]['id']\n",
    "        payload.pop('query', None)\n",
    "        movie_url = 'http://api.themoviedb.org/3/movie/{:}/images'.format(movie_id)\n",
    "        response = requests.get(movie_url, params=payload, headers=headers)\n",
    "        file_path = json.loads(response.text)['posters'][0]['file_path']\n",
    "        \n",
    "    return base_url + file_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b9860b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in movie data\n",
    "idx_to_movie = {}\n",
    "with open('u.item', 'r') as f:\n",
    "    for line in f.readlines():\n",
    "        info = line.split('|')\n",
    "        idx_to_movie[int(info[0])-1] = info[4]\n",
    "        \n",
    "def top_k_movies(similarity, mapper, movie_idx, k=6):\n",
    "    return [mapper[x] for x in np.argsort(similarity[movie_idx,:])[:-k-1:-1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eacf9b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 40 # Billy Madison\n",
    "movies = top_k_movies(item_similarity, idx_to_movie, idx)\n",
    "posters = tuple(Image(url=get_poster(movie, base_url)) for movie in movies)\n",
    "display(*posters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e1e6faf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import pairwise_distances\n",
    "# Convert from distance to similarity\n",
    "item_correlation = 1 - pairwise_distances(train.T, metric='correlation')\n",
    "item_correlation[np.isnan(item_correlation)] = 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aaa9d38",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 0 # Toy Story\n",
    "movies = top_k_movies(item_correlation, idx_to_movie, idx)\n",
    "posters = tuple(Image(url=get_poster(movie, base_url)) for movie in movies)\n",
    "display(*posters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "505510b1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
